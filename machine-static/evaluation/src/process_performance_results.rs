use evaluation::{BenchMarkInput, create_directory, prepare_files_in_directory};
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};
use walkdir::WalkDir;
use core::fmt;
use std::{collections::BTreeMap, ffi::OsStr, fs::File, path::{Path, PathBuf}};
use clap::{Parser, ValueEnum};
use anyhow::{Context, Error, Result};

#[derive(Parser)]
#[command(version, about, long_about = None)]
pub struct CliProcessPerf {
    // Read inputs from this directory -- results generated by criterion.
    #[arg(short, long)]
    pub result_dir: PathBuf,

    // Read benchmark suite from this directory.
    #[arg(short, long)]
    pub benchmark_dir: PathBuf,

    // Store outputs here.
    #[arg(short, long)]
    pub output_path: PathBuf,

    #[arg(short, long)]
    pub unit: Unit,
}

#[derive(ValueEnum, Debug, Serialize, Deserialize, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum Unit {
    NS,
    MUS
}

// Get divisor to go from ns to unit
impl Unit {
    fn get_divisor(&self) -> f64 {
        match self {
            Unit::NS => 1.0,
            Unit::MUS => 1000.0
        }
    }
}

impl fmt::Display for Unit {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Unit::NS => write!(f, "ns"),
            Unit::MUS => write!(f, "mus")
        }
    }
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/estimate.rs
#[derive(Clone, PartialEq, Deserialize, Serialize, Debug)]
pub struct ConfidenceInterval {
    pub confidence_level: f64,
    pub lower_bound: f64,
    pub upper_bound: f64,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/estimate.rs
#[derive(Clone, PartialEq, Deserialize, Serialize, Debug)]
pub struct Estimate {
    /// The confidence interval for this estimate
    pub confidence_interval: ConfidenceInterval,
    pub point_estimate: f64,
    /// The standard error of this estimate
    pub standard_error: f64,
}

impl Estimate {
    // Transform to FlattenedEstimate dividing converting to indicated unit while doing so. Estimate is in ns.
    pub fn to_flattened_estimate(&self, unit: &Unit) -> FlattenedEstimate {
        FlattenedEstimate {
            confidence_interval_lower_bound: self.confidence_interval.lower_bound / unit.get_divisor(),
            confidence_interval_upper_bound: self.confidence_interval.upper_bound / unit.get_divisor(),
            confidence_level: self.confidence_interval.confidence_level,
            point_estimate: self.point_estimate / unit.get_divisor(),
            standard_error: self.standard_error / unit.get_divisor(),
            unit: unit.clone()
        }
    }
}

#[derive(Clone, PartialEq, Deserialize, Serialize, Debug)]
pub struct FlattenedEstimate {
    confidence_interval_lower_bound: f64,
    confidence_interval_upper_bound: f64,
    confidence_level: f64,
    point_estimate: f64,
    standard_error: f64,
    unit: Unit,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/estimate.rs
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Estimates {
    pub mean: Estimate,
    pub median: Estimate,
    pub median_abs_dev: Estimate,
    pub slope: Option<Estimate>,
    pub std_dev: Estimate,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/estimate.rs
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChangeEstimates {
    pub mean: Estimate,
    pub median: Estimate,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/connection.rs
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum Throughput {
    Bytes(u64),
    BytesDecimal(u64),
    Elements(u64),
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/model.rs
#[derive(Debug, Serialize, Deserialize, Clone)]
pub enum ChangeDirection {
    NoChange,
    NotSignificant,
    Improved,
    Regressed,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/model.rs
// Data stored on disk when running benchmarks
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SavedStatistics {
    // The timestamp of when these measurements were saved.
    pub datetime: DateTime<Utc>,
    // The number of iterations in each sample
    pub iterations: Vec<f64>,
    // The measured values from each sample
    pub values: Vec<f64>,
    // The average values from each sample, ie. values / iterations
    pub avg_values: Vec<f64>,
    // The statistical estimates from this run
    pub estimates: Estimates,
    // The throughput of this run
    pub throughput: Option<Throughput>,
    // The statistical differences compared to the last run. We save these so we don't have to
    // recompute them later for the history report.
    pub changes: Option<ChangeEstimates>,
    // Was the change (if any) significant?
    pub change_direction: Option<ChangeDirection>,

    // An optional user-provided identifier string. This might be a version control commit ID or
    // something custom
    pub history_id: Option<String>,
    // An optional user-provided description. This might be a version control commit message or
    // something custom.
    pub history_description: Option<String>,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/model.rs
#[derive(Debug, Deserialize, Serialize)]
pub struct SavedBenchmarkId {
    group_id: String,
    function_id: Option<String>,
    value_str: Option<String>,
    throughput: Option<Throughput>,
}

// From https://github.com/bheisler/cargo-criterion/blob/main/src/model.rs
#[derive(Debug, Serialize, Deserialize)]
struct BenchmarkRecord {
    id: SavedBenchmarkId,
    latest_record: PathBuf,
}


#[derive(Debug, Serialize, Deserialize)]
struct NamedSavedStatistics {
    function_id: Option<String>,    // Name of benchmarked function e.g. Exact or Algorithm 1
    value_str: Option<String>,      // Name of input to benchmarked function, for us, the number of states in the composition.
    measurement_fname: PathBuf,      // File containing statistics
    saved_statistics: SavedStatistics
}

// Struct containing some of the fields from a NamedSavedStatistics + some info about input sample protocol used to generate it
// in a format we can easily write to csv. Slope from Estimates not included because it is often None (we'd have run experiments for MUCH longer to get it for the big protocols.)
#[derive(Debug, Serialize, Deserialize)]
struct FlattenedMeasurement {
    number_of_edges: usize,
    state_space_size: usize,
    algorithm: String,
    unit: Unit,

    mean_confidence_interval_lower_bound: f64,
    mean_confidence_interval_upper_bound: f64,
    mean_confidence_level: f64,
    mean_point_estimate: f64,
    mean_standard_error: f64,

    median_confidence_interval_lower_bound: f64,
    median_confidence_interval_upper_bound: f64,
    median_confidence_level: f64,
    median_point_estimate: f64,
    median_standard_error: f64,

    median_abs_dev_confidence_interval_lower_bound: f64,
    median_abs_dev_confidence_interval_upper_bound: f64,
    median_abs_dev_confidence_level: f64,
    median_abs_dev_point_estimate: f64,
    median_abs_dev_standard_error: f64,

    std_dev_confidence_interval_lower_bound: f64,
    std_dev_confidence_interval_upper_bound: f64,
    std_dev_confidence_level: f64,
    std_dev_point_estimate: f64,
    std_dev_standard_error: f64,
}

fn flatten_measurement(named_saved_statistics: NamedSavedStatistics, benchmark_input: &BenchMarkInput, unit: Unit) -> Result<FlattenedMeasurement> {
    let number_of_edges = benchmark_input.number_of_edges;
    let state_space_size: usize = named_saved_statistics.value_str
        .ok_or(Error::msg("Error reading value_str from NamedSavedStatistics"))?.parse()?;
    //let state_space_size: usize = state_space_size_str.parse().ok()?;
    let algorithm = named_saved_statistics.function_id.ok_or(Error::msg("Error reading function_id from NamedSavedStatistics"))?;

    let mean_flattened_estimate = named_saved_statistics.saved_statistics.estimates.mean.to_flattened_estimate(&unit);
    let median_flattened_estimate = named_saved_statistics.saved_statistics.estimates.median.to_flattened_estimate(&unit);
    let median_abs_dev_flattened_estimate = named_saved_statistics.saved_statistics.estimates.median_abs_dev.to_flattened_estimate(&unit);
    let std_dev_flattened_estimate = named_saved_statistics.saved_statistics.estimates.std_dev.to_flattened_estimate(&unit);

    Ok(
        FlattenedMeasurement {
            number_of_edges,
            state_space_size,
            algorithm,
            unit,

            mean_confidence_interval_lower_bound: mean_flattened_estimate.confidence_interval_lower_bound,
            mean_confidence_interval_upper_bound: mean_flattened_estimate.confidence_interval_upper_bound,
            mean_confidence_level: mean_flattened_estimate.confidence_level,
            mean_point_estimate: mean_flattened_estimate.point_estimate,
            mean_standard_error: mean_flattened_estimate.standard_error,

            median_confidence_interval_lower_bound: median_flattened_estimate.confidence_interval_lower_bound,
            median_confidence_interval_upper_bound: median_flattened_estimate.confidence_interval_upper_bound,
            median_confidence_level: median_flattened_estimate.confidence_level,
            median_point_estimate: median_flattened_estimate.point_estimate,
            median_standard_error: median_flattened_estimate.standard_error,

            median_abs_dev_confidence_interval_lower_bound: median_abs_dev_flattened_estimate.confidence_interval_lower_bound,
            median_abs_dev_confidence_interval_upper_bound: median_abs_dev_flattened_estimate.confidence_interval_upper_bound,
            median_abs_dev_confidence_level: median_abs_dev_flattened_estimate.confidence_level,
            median_abs_dev_point_estimate: median_abs_dev_flattened_estimate.point_estimate,
            median_abs_dev_standard_error: median_abs_dev_flattened_estimate.standard_error,

            std_dev_confidence_interval_lower_bound: std_dev_flattened_estimate.confidence_interval_lower_bound,
            std_dev_confidence_interval_upper_bound: std_dev_flattened_estimate.confidence_interval_upper_bound,
            std_dev_confidence_level: std_dev_flattened_estimate.confidence_level,
            std_dev_point_estimate: std_dev_flattened_estimate.point_estimate,
            std_dev_standard_error: std_dev_flattened_estimate.standard_error
        }
    )
}

// Should be called with a path contatining criterion data directories (directories containing cbors with benchmark results).
// Adapted from https://github.com/bheisler/cargo-criterion/blob/main/src/model.rs/: load impl for Model
fn load_latest_measurements(path: &Path) -> Result<Vec<NamedSavedStatistics>> {
    let mut stats = Vec::new();
    for entry in WalkDir::new(path)
        .into_iter()
        // Ignore errors.
        .filter_map(::std::result::Result::ok)
        .filter(|entry| entry.file_name() == OsStr::new("benchmark.cbor"))
    {
        let statistics = load_latest(entry.path())?;
        stats.push(statistics);
    }

    Ok(stats)
}

// Benchmark path should be the path of a "benchmark.cbor" file
// Adapted from  https://github.com/bheisler/cargo-criterion/blob/main/src/model.rs/: load_stored_benchmark impl for model
fn load_latest(benchmark_path: &Path) -> Result<NamedSavedStatistics> {
    if !benchmark_path.is_file() {
        return Err(Error::msg(format!("Invalid benchmark_path: {} is not a file", benchmark_path.display())));
    }
    let mut benchmark_file = File::open(benchmark_path)
        .with_context(|| format!("Failed to open benchmark file {:?}", benchmark_path))?;
    let benchmark_record: BenchmarkRecord = serde_cbor::from_reader(&mut benchmark_file)
        .with_context(|| format!("Failed to read benchmark file {:?}", benchmark_path))?;

    let measurement_path = benchmark_path.with_file_name(benchmark_record.latest_record);
    if !measurement_path.is_file() {
        return Err(Error::msg(format!("Error: latest measurement {} is not a file", measurement_path.display())));
    }
    let mut measurement_file = File::open(&measurement_path)
        .with_context(|| format!("Failed to open measurement file {:?}", measurement_path))?;
    let saved_stats: SavedStatistics = serde_cbor::from_reader(&mut measurement_file)
        .with_context(|| format!("Failed to read measurement file {:?}", measurement_path))?;

    return Ok(
        NamedSavedStatistics {
            function_id: benchmark_record.id.function_id,
            value_str: benchmark_record.id.value_str,
            measurement_fname: measurement_path,
            saved_statistics: saved_stats
        }
    )
}

fn write_csv<S: Serialize> (processed: Vec<S>, output_file: &Path) -> Result<()> {
    let mut wtr = csv::Writer::from_path(output_file)?;

    for record in processed {
        wtr.serialize(record)?;
    }
    wtr.flush()?;
    Ok(())
}

// Read all benchmark samples at path and return them as a map from number of states to samples
fn benchmark_inputs(path: &Path) -> BTreeMap<usize, BenchMarkInput> {
    prepare_files_in_directory(path)
        .into_iter()
        .collect()
}

// Transform a vec of NamedSavedStatistics into a vec of FlattenedMeasurements
fn flatten_measurements(measurements: Vec<NamedSavedStatistics>, benchmarks: BTreeMap<usize, BenchMarkInput>, unit: &Unit) -> Result<Vec<FlattenedMeasurement>> {
    let mapper = |named_saved_statistics: NamedSavedStatistics| -> Result<FlattenedMeasurement> {
        let state_space_size: usize = named_saved_statistics.value_str
            .clone()
            .ok_or(Error::msg("Error reading value_str from NamedSavedStatistics"))?.parse()?;
        let benchmark_input = benchmarks.get(&state_space_size)
            .ok_or(Error::msg(format!("Error: no benchmark_input for key {}", state_space_size)))?;

        flatten_measurement(named_saved_statistics, benchmark_input, unit.clone())

    };

    measurements
        .into_iter()
        .map(mapper)
        .collect()
}

fn partition_measurements(flattened_measurements: Vec<FlattenedMeasurement>) -> BTreeMap<String, Vec<FlattenedMeasurement>> {
    let mut partitioned: BTreeMap<String, Vec<FlattenedMeasurement>> = BTreeMap::new();

    for measurement in flattened_measurements {
        partitioned
            .entry(measurement.algorithm.clone())
            .or_insert_with(|| Vec::new())
            .push(measurement);
    }


    partitioned
}

// partition measurements by algorithm used in benchmark (i.e. algorithm 1 or exact) and write each such partition to its own file.
fn process_performance_results(result_dir: &Path, benchmark_dir: &Path, output_path: &Path, unit: &Unit) -> Result<()> {
    let prefix = output_path.parent().ok_or(Error::msg("Error: invalid output parent directory"))?;
    create_directory(prefix);
    let fname = output_path
        .file_name()
        .ok_or(Error::msg(format!("Could not get file name for {}", output_path.display())))?
        .to_str().ok_or(Error::msg(format!("{} is not valid Unicode", output_path.display())))?;

    let benchmarks = benchmark_inputs(benchmark_dir);
    let measurements = load_latest_measurements(&result_dir)?;
    let flattened_measurements = flatten_measurements(measurements, benchmarks, unit)?;

    let partitioned_measurements = partition_measurements(flattened_measurements);

    for (algorithm, flat_measurement) in partitioned_measurements {
        let informative_fname = format!("{}_{}_{}", algorithm.replace(" ", "_"), unit.clone(), fname);
        write_csv(flat_measurement, &output_path.with_file_name(informative_fname))?;
    }

    Ok(())
}

fn main() {
    let cli = CliProcessPerf::parse();
    let result_dir = cli.result_dir;
    let benchmark_dir = cli.benchmark_dir;
    let output_path = cli.output_path;
    let unit = cli.unit;

    if let Err(e) =  process_performance_results(&result_dir, &benchmark_dir, &output_path, &unit) {
        println!("Error processing performance results: {}", e);
    }
}

/*

    let mut partitioned: BTreeMap<String, Vec<FlattenedMeasurement>> = BTreeMap::new();

    for measurement in flattened_measurements {
        partitioned.entry(measurement.algorithm.clone())
            .or_insert_with(Vec::new)
            .push(measurement);
    }

    partitioned

        for measurement in flattened_measurements{
        partitioned
            .entry(measurement.algorithm.clone())
            .and_modify(|measurements| measurements.push(measurement.clone()))
            .or_insert_with(|| vec![measurement]);
    }
*/